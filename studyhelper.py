import os
import streamlit as st
from io import BytesIO
from dotenv import load_dotenv
# OpenAI ëª¨ë“ˆì´ ì—†ì–´ë„ Gemini APIë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None
from pathlib import Path
import docx2txt
import pdfplumber
from pptx import Presentation
import random
import subprocess
import nltk
from nltk.corpus import stopwords
import google.generativeai as genai
import PIL.Image
import requests
from concurrent.futures import ThreadPoolExecutor

###############################################################################
# NLTK ì„¤ì • ë° ë¶ˆìš©ì–´ ì²˜ë¦¬ (ìºì‹± ì ìš©)
###############################################################################
nltk_data_dir = "/tmp/nltk_data"
os.makedirs(nltk_data_dir, exist_ok=True)
nltk.data.path.append(nltk_data_dir)

@st.cache_data(show_spinner=False)
def get_stopwords():
    try:
        sw = stopwords.words("english")
    except LookupError:
        nltk.download("stopwords", download_dir=nltk_data_dir)
        sw = stopwords.words("english")
    return set(sw)

english_stopwords = get_stopwords()
korean_stopwords = {"ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë“¤", "ë°", "ë”"}
final_stopwords = english_stopwords.union(korean_stopwords)

###############################################################################
# í™˜ê²½ ë³€ìˆ˜ ë° API í‚¤ ì„¤ì •
###############################################################################
dotenv_path = Path(".env")
load_dotenv(dotenv_path=dotenv_path)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
USE_GEMINI_ALWAYS = os.getenv("USE_GEMINI_ALWAYS", "False").lower() == "true"

# OpenAI APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ê±°ë‚˜ USE_GEMINI_ALWAYSê°€ Trueì´ë©´ Gemini API ì‚¬ìš©
if USE_GEMINI_ALWAYS or not OPENAI_API_KEY or OpenAI is None:
    use_gemini_always = True
else:
    use_gemini_always = False
    client = OpenAI(api_key=OPENAI_API_KEY)

###############################################################################
# OpenAI API ë§ˆì´ê·¸ë ˆì´ì…˜ (í•„ìš” ì‹œ)
###############################################################################
def migrate_openai_api():
    try:
        subprocess.run(["openai", "migrate"], capture_output=True, text=True, check=True)
        st.info("OpenAI API ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ. ì•± ì¬ì‹œì‘ í›„ ì‚¬ìš©í•˜ì„¸ìš”.")
        st.stop()
    except Exception as e:
        st.error(f"API ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
        st.stop()

###############################################################################
# GPT API í˜¸ì¶œ í•¨ìˆ˜ (OpenAI ë° Gemini)
###############################################################################
def ask_gpt(messages, model_name="gpt-4", temperature=0.7):
    """OpenAI GPT í˜¸ì¶œ, ì‹¤íŒ¨ ì‹œ Gemini API í˜¸ì¶œ."""
    if use_gemini_always:
        return ask_gemini(messages, temperature=temperature)
    try:
        resp = client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=temperature,
        )
        return resp.choices[0].message.content.strip()
    except Exception:
        return ask_gemini(messages, temperature=temperature)

def ask_gemini(messages, temperature=0.7):
    """
    Gemini API í˜¸ì¶œ í•¨ìˆ˜ (GenerativeModel ë°©ì‹).
    ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê²°í•©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³  generate_content()ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        system_message = next((m["content"] for m in messages if m["role"] == "system"), "")
        user_message = next((m["content"] for m in messages if m["role"] == "user"), "")
        prompt = f"{system_message}\n\nì‚¬ìš©ì ì§ˆë¬¸: {user_message}"
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(
            prompt,
            generation_config={"temperature": temperature}
        )
        return response.text.strip()
    except Exception as e:
        st.error(f"ğŸš¨ Gemini API (GenerativeModel ë°©ì‹) í˜¸ì¶œ ì—ëŸ¬: {e}")
        return ""

###############################################################################
# Gemini API ì˜ˆì œ (OpenAI ë°©ì‹ í˜¸ì¶œ)
###############################################################################
def gemini_api_example():
    """
    OpenAI ëª¨ë“ˆì„ ì´ìš©í•˜ì—¬ Gemini APIë¥¼ í˜¸ì¶œí•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.
    base_urlì„ Gemini API ì—”ë“œí¬ì¸íŠ¸ë¡œ ì„¤ì •í•˜ì—¬ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        example_client = OpenAI(
            api_key=GEMINI_API_KEY,
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
        )
        response = example_client.chat.completions.create(
            model="gemini-2.0-flash",
            n=1,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Explain to me how AI works"}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Gemini API ì˜ˆì œ í˜¸ì¶œ ì—ëŸ¬: {e}"

###############################################################################
# ë¬¸ì„œ ë° ì´ë¯¸ì§€ íŒŒì‹± í•¨ìˆ˜ (ìºì‹± ì ìš©)
###############################################################################
@st.cache_data(show_spinner=False)
def parse_docx(file_bytes):
    try:
        return docx2txt.process(BytesIO(file_bytes))
    except Exception:
        return "ğŸ“„ DOCX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

@st.cache_data(show_spinner=False)
def parse_pdf(file_bytes):
    text_list = []
    try:
        with pdfplumber.open(BytesIO(file_bytes)) as pdf:
            for page in pdf.pages:
                text_list.append(page.extract_text() or "")
        return "\n".join(text_list)
    except Exception:
        return "ğŸ“„ PDF íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

@st.cache_data(show_spinner=False)
def parse_ppt(file_bytes):
    text_list = []
    try:
        prs = Presentation(BytesIO(file_bytes))
        for slide in prs.slides:
            for shape in slide.shapes:
                if shape.has_text_frame:
                    text_list.append(shape.text)
        return "\n".join(text_list)
    except Exception:
        return "ğŸ“„ PPTX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

@st.cache_data(show_spinner=False)
def analyze_image(file_bytes):
    try:
        image = PIL.Image.open(BytesIO(file_bytes))
        width, height = image.size
        return f"ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼: ì´ë¯¸ì§€ í¬ê¸°ëŠ” {width}x{height} í”½ì…€ì…ë‹ˆë‹¤."
    except Exception as e:
        return f"ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}"

def analyze_file(fileinfo):
    ext = fileinfo["ext"]
    data = fileinfo["data"]
    if ext == "docx":
        return parse_docx(data)
    elif ext == "pdf":
        return parse_pdf(data)
    elif ext == "pptx":
        return parse_ppt(data)
    elif ext in ["png", "jpg", "jpeg"]:
        return analyze_image(data)
    else:
        return "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. (PDF, PPTX, DOCX, PNG, JPG, JPEG ì§€ì›)"

###############################################################################
# ì—¬ëŸ¬ íŒŒì¼ ë³‘í•© (ë³‘ë ¬ ì²˜ë¦¬ ì ìš©)
###############################################################################
def merge_documents(file_list):
    def process_file(file):
        file_bytes = file.getvalue()
        fileinfo = {
            "name": file.name,
            "ext": file.name.split(".")[-1].lower(),
            "data": file_bytes
        }
        return f"\n\n--- {file.name} ---\n\n" + analyze_file(fileinfo)
    
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(process_file, file_list))
    return "".join(results)

###############################################################################
# GPT ë¬¸ì„œ ë¶„ì„, ì§ˆë¬¸, ë§ì¶¤ë²• ìˆ˜ì • ê¸°ëŠ¥
###############################################################################
def gpt_document_review(text):
    summary_prompt = [
        {"role": "system", "content": "ì£¼ì–´ì§„ íŒŒì¼ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ì£¼ìš” ë‚´ìš©ì„ ì •ë¦¬í•˜ì„¸ìš”."},
        {"role": "user", "content": text}
    ]
    summary = ask_gpt(summary_prompt)
    question_prompt = [
        {"role": "system", "content": "ì£¼ì–´ì§„ íŒŒì¼ ë‚´ìš©ì„ ê²€í† í•˜ê³ , ì‚¬ìš©ìê°€ ìˆ˜ì •í•˜ê±°ë‚˜ ê³ ë ¤í•´ì•¼ í•  ì§ˆë¬¸ì„ 3ê°€ì§€ ì œì‹œí•˜ì„¸ìš”."},
        {"role": "user", "content": text}
    ]
    questions = ask_gpt(question_prompt)
    correction_prompt = [
        {"role": "system", "content": "ì´ íŒŒì¼ì—ì„œ ë§ì¶¤ë²•ê³¼ ë¬¸ë²• ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•˜ê³ , ìˆ˜ì •í•œ ë¶€ë¶„ì„ ê°•ì¡°í•˜ì„¸ìš”."},
        {"role": "user", "content": text}
    ]
    corrections = ask_gpt(correction_prompt)
    return summary, questions, corrections

###############################################################################
# GPT/AI ì±„íŒ… ë° íŒŒì¼ ë¶„ì„ íƒ­
###############################################################################
def gpt_chat_tab():
    st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ AIê°€ ìë™ìœ¼ë¡œ íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤. íŒŒì¼ ì—…ë¡œë“œ ì—†ì´ë„ ììœ ë¡­ê²Œ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    # íŒŒì¼ ì—…ë¡œë“œ: ì„ íƒ ì‚¬í•­
    uploaded_files = st.file_uploader(
        "ğŸ“ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ì„ íƒ: PDF, PPTX, DOCX, PNG, JPG, JPEG ì§€ì›)",
        type=["pdf", "pptx", "docx", "png", "jpg", "jpeg"],
        accept_multiple_files=True
    )
    if uploaded_files is not None and len(uploaded_files) > 0:
        with st.spinner("íŒŒì¼ì„ ë¶„ì„ ì¤‘..."):
            document_text = merge_documents(uploaded_files)
            summary, questions, corrections = gpt_document_review(document_text)
            st.session_state.document_text = document_text
            st.session_state.summary = summary
            st.session_state.questions = questions
            st.session_state.corrections = corrections
    # íŒŒì¼ ë¯¸ì—…ë¡œë“œ ì‹œì—ë„ ëŒ€í™”ê°€ ê°€ëŠ¥í•˜ë„ë¡ ê¸°ë³¸ê°’ ìœ ì§€
    if "document_text" not in st.session_state:
        st.session_state.document_text = ""
    st.subheader("ğŸ’¬ AIì™€ ëŒ€í™”í•˜ê¸°")
    user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key="chat_input")
    if st.button("ì „ì†¡"):
        if user_input.strip():
            st.session_state.chat_history.append({"role": "user", "content": user_input})
            prompt_context = f"íŒŒì¼ ë‚´ìš©: {st.session_state.document_text}" if st.session_state.document_text else "ììœ ë¡œìš´ ëŒ€í™”"
            chat_prompt = [
                {"role": "system", "content": f"ë‹¹ì‹ ì€ {prompt_context}ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": user_input}
            ]
            ai_response = ask_gpt(chat_prompt)
            st.session_state.chat_history.append({"role": "assistant", "content": ai_response})
        else:
            st.error("ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    for message in st.session_state.chat_history:
        if message["role"] == "user":
            st.write(f"**ì‚¬ìš©ì**: {message['content']}")
        else:
            st.write(f"**AI**: {message['content']}")
    # Gemini API ì˜ˆì œ ë²„íŠ¼ ì¶”ê°€ (OpenAI ë°©ì‹)
    if st.button("Gemini API ì˜ˆì œ (OpenAI ë°©ì‹)"):
        result = gemini_api_example()
        st.write("Gemini API ì˜ˆì œ ê²°ê³¼:", result)

###############################################################################
# ì»¤ë®¤ë‹ˆí‹° íƒ­ (ìµëª… ëŒ“ê¸€ ë° í˜‘ì—…)
###############################################################################
def community_tab():
    st.header("ğŸŒ ì»¤ë®¤ë‹ˆí‹° (íŒŒì¼ ê³µìœ  ë° í† ë¡ )")
    st.info("""
**ì»¤ë®¤ë‹ˆí‹° ì‚¬ìš©ë²•**
- ê²Œì‹œê¸€ ì‘ì„± ì‹œ ì œëª©, ë‚´ìš© ë° íŒŒì¼(ì§€ì›: PDF, PPTX, DOCX, ì´ë¯¸ì§€)ì„ ì²¨ë¶€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê²Œì‹œê¸€ ê²€ìƒ‰ê³¼ ìµëª… ëŒ“ê¸€ ê¸°ëŠ¥ì„ í†µí•´ íŒŒì¼ ë° ë¶„ì„ ê²°ê³¼ì— ëŒ€í•´ í† ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    search_query = st.text_input("ğŸ” ê²€ìƒ‰ (ì œëª© ë˜ëŠ” ë‚´ìš© ì…ë ¥)")
    if "community_posts" not in st.session_state:
        st.session_state.community_posts = []
    st.subheader("ğŸ“¤ ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì‘ì„±")
    title = st.text_input("ì œëª©")
    content = st.text_area("ë‚´ìš©")
    uploaded_files = st.file_uploader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", type=["pdf", "pptx", "docx", "png", "jpg", "jpeg"], accept_multiple_files=True)
    if st.button("âœ… ê²Œì‹œê¸€ ë“±ë¡"):
        if title.strip() and content.strip():
            files_info = []
            if uploaded_files:
                for uf in uploaded_files:
                    file_bytes = uf.getvalue()
                    ext = uf.name.split(".")[-1].lower()
                    files_info.append({"name": uf.name, "ext": ext, "data": file_bytes})
            new_post = {"title": title, "content": content, "files": files_info, "comments": []}
            st.session_state.community_posts.append(new_post)
            st.success("âœ… ê²Œì‹œê¸€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            st.error("ì œëª©ê³¼ ë‚´ìš©ì„ ëª¨ë‘ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    st.subheader("ğŸ“œ ê²Œì‹œê¸€ ëª©ë¡")
    for idx, post in enumerate(st.session_state.community_posts):
        if not search_query or search_query.lower() in post["title"].lower() or search_query.lower() in post["content"].lower():
            with st.expander(f"{idx+1}. {post['title']}"):
                st.write(post["content"])
                # ìµëª… ëŒ“ê¸€ ì‘ì„±
                comment = st.text_input(f"ğŸ’¬ ëŒ“ê¸€ ì‘ì„± (ìµëª…)", key=f"comment_{idx}")
                if st.button("ëŒ“ê¸€ ë“±ë¡", key=f"comment_btn_{idx}"):
                    if comment.strip():
                        st.session_state.community_posts[idx]["comments"].append(f"ìµëª…_{random.randint(100,999)}: {comment}")
                    else:
                        st.error("ëŒ“ê¸€ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                for c in post["comments"]:
                    st.write(c)

###############################################################################
# ë©”ì¸ ì‹¤í–‰ ë° ì‚¬ìš©ë²• ì•ˆë‚´
###############################################################################
def main():
    st.title("ğŸ“š Thinkhelper")
    st.markdown("""
**Thinkhelper**ëŠ” AI ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼(ë¬¸ì„œ ë° ì´ë¯¸ì§€)ì„ ìë™ ë¶„ì„í•˜ì—¬ ìš”ì•½, ìˆ˜ì • ì œì•ˆ, ê°œì„  ì‚¬í•­ì„ ì œê³µí•©ë‹ˆë‹¤.
ë˜í•œ, ì»¤ë®¤ë‹ˆí‹° íƒ­ì„ í†µí•´ íŒŒì¼ì„ ê³µìœ í•˜ê³  ìµëª…ìœ¼ë¡œ í† ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì‚¬ìš©ë²•**
- **GPT ë¬¸ì„œ ë¶„ì„ íƒ­:** íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼(ìš”ì•½, ì§ˆë¬¸, ìˆ˜ì • ì‚¬í•­)ë¥¼ í™•ì¸í•˜ê±°ë‚˜, íŒŒì¼ ì—†ì´ë„ ììœ ë¡­ê²Œ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ì»¤ë®¤ë‹ˆí‹° íƒ­:** ê²Œì‹œê¸€ì„ ë“±ë¡í•˜ê³  ìµëª… ëŒ“ê¸€ì„ í†µí•´ íŒŒì¼ ë° ë¶„ì„ ê²°ê³¼ì— ëŒ€í•´ í† ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    tab = st.sidebar.radio("ğŸ” ë©”ë‰´ ì„ íƒ", ("GPT ë¬¸ì„œ ë¶„ì„", "ì»¤ë®¤ë‹ˆí‹°"))
    if tab == "GPT ë¬¸ì„œ ë¶„ì„":
        gpt_chat_tab()
    else:
        community_tab()

if __name__ == "__main__":
    main()

st.markdown("""
---
**ì €ì‘ê¶Œ ì£¼ì˜ ë¬¸êµ¬**

- **ì½”ë“œ ì‚¬ìš©**: ì´ ì†ŒìŠ¤ ì½”ë“œëŠ” ì €ì‘ê¶Œë²•ì— ì˜í•´ ë³´í˜¸ë©ë‹ˆë‹¤. ë¬´ë‹¨ ë³µì œ, ë°°í¬, ìˆ˜ì • ë˜ëŠ” ìƒì—…ì  ì‚¬ìš©ì€ ê¸ˆì§€ë©ë‹ˆë‹¤. ê°œì¸ì , ë¹„ìƒì—…ì  ìš©ë„ë¡œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ì‚¬ìš© ì‹œ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œê¸°í•´ì•¼ í•©ë‹ˆë‹¤.
- **íŒŒì¼ ì—…ë¡œë“œ**: íŒŒì¼ ì—…ë¡œë“œ ì‹œ ì €ì‘ê¶Œì— ìœ ì˜í•´ ì£¼ì„¸ìš”. ì €ì‘ê¶Œ ì¹¨í•´ ë¬¸ì œê°€ ë°œìƒí•  ê²½ìš°, ë³¸ ì„œë¹„ìŠ¤ëŠ” ì±…ì„ì„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
""")

import os
import streamlit as st
from io import BytesIO
from dotenv import load_dotenv
import openai
from pathlib import Path
import docx2txt
import pdfplumber
from pptx import Presentation
import random
import subprocess
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

###############################################################################
# NLTK ì„¤ì • (ë¶ˆìš©ì–´, punkt, averaged_perceptron_tagger ìë™ ë‹¤ìš´ë¡œë“œ)
###############################################################################
nltk_data_dir = "/tmp/nltk_data"
os.makedirs(nltk_data_dir, exist_ok=True)
nltk.data.path.append(nltk_data_dir)

try:
    stopwords.words("english")
except LookupError:
    nltk.download("stopwords", download_dir=nltk_data_dir)

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', download_dir=nltk_data_dir)

try:
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_dir)

korean_stopwords = ["ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë“¤", "ë°", "ë”"]
english_stopwords = set(stopwords.words("english"))
final_stopwords = english_stopwords.union(set(korean_stopwords))

###############################################################################
# í™˜ê²½ ë³€ìˆ˜ & OpenAI API ì„¤ì •
###############################################################################
if not os.getenv("OPENAI_API_KEY"):
    dotenv_path = Path(".env")
    load_dotenv(dotenv_path=dotenv_path)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("ğŸš¨ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œì»¬ì—ì„œëŠ” .env íŒŒì¼ì„, Streamlit Cloudì—ì„œëŠ” 'Settings'ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()

openai.api_key = OPENAI_API_KEY

###############################################################################
# GPT API í˜¸ì¶œ í•¨ìˆ˜
###############################################################################
def ask_gpt(messages, model_name="gpt-4", temperature=0.7):
    try:
        resp = openai.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=temperature,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        st.error(f"ğŸš¨ OpenAI API í˜¸ì¶œ ì—ëŸ¬: {e}")
        return ""

###############################################################################
# ë¬¸ì„œ ë¶„ì„ í•¨ìˆ˜ (PDF, PPTX, DOCX)
###############################################################################
def parse_docx(file_bytes):
    try:
        return docx2txt.process(BytesIO(file_bytes))
    except Exception:
        return "ğŸ“„ DOCX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

def parse_pdf(file_bytes):
    text_list = []
    try:
        with pdfplumber.open(BytesIO(file_bytes)) as pdf:
            for page in pdf.pages:
                text_list.append(page.extract_text() or "")
        return "\n".join(text_list)
    except Exception:
        return "ğŸ“„ PDF íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

def parse_ppt(file_bytes):
    text_list = []
    try:
        prs = Presentation(BytesIO(file_bytes))
        for slide in prs.slides:
            for shape in slide.shapes:
                if shape.has_text_frame:
                    text_list.append(shape.text)
        return "\n".join(text_list)
    except Exception:
        return "ğŸ“„ PPTX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

def analyze_file(fileinfo):
    ext = fileinfo["ext"]
    data = fileinfo["data"]
    if ext == "docx":
        return parse_docx(data)
    elif ext == "pdf":
        return parse_pdf(data)
    elif ext == "pptx":
        return parse_ppt(data)
    else:
        return "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."

###############################################################################
# ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ í•¨ìˆ˜
###############################################################################
def extract_important_words(text, top_n=10):
    words = word_tokenize(text.lower())
    words = [word for word in words if word.isalnum() and word not in final_stopwords]
    tagged = nltk.pos_tag(words)
    nouns = [word for word, pos in tagged if pos.startswith('N')]
    freq_dist = nltk.FreqDist(nouns)
    return [word for word, _ in freq_dist.most_common(top_n)]

###############################################################################
# GPT ë¬¸ì„œ ë¶„ì„ í•¨ìˆ˜
###############################################################################
def gpt_document_review(text):
    summary_prompt = [
        {"role": "system", "content": "ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ê³  ì£¼ìš” ë‚´ìš©ì„ ì •ë¦¬í•˜ì„¸ìš”."},
        {"role": "user", "content": text}
    ]
    summary = ask_gpt(summary_prompt)

    question_prompt = [
        {"role": "system", "content": "ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê²€í† í•˜ê³ , ì‚¬ìš©ìê°€ ìˆ˜ì •í•˜ê±°ë‚˜ ê³ ë ¤í•´ì•¼ í•  ì§ˆë¬¸ì„ 3ê°€ì§€ ì œì‹œí•˜ì„¸ìš”."},
        {"role": "user", "content": text}
    ]
    questions = ask_gpt(question_prompt)

    correction_prompt = [
        {"role": "system", "content": "ì´ ë¬¸ì„œì—ì„œ ë§ì¶¤ë²•ê³¼ ë¬¸ë²• ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•˜ê³ , ìˆ˜ì •í•œ ë¶€ë¶„ì„ ê°•ì¡°í•˜ì„¸ìš”."},
        {"role": "user", "content": text}
    ]
    corrections = ask_gpt(correction_prompt)

    return summary, questions, corrections

###############################################################################
# ëŒ€í™”í˜• ì±„íŒ… ê¸°ëŠ¥ (íŒŒì¼ ì—…ë¡œë“œ ì‹œ ìë™ í‘œì‹œ)
###############################################################################
def show_interactive_chat(document_text):
    if 'conversation' not in st.session_state:
        st.session_state.conversation = []
    
    st.info("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”. GPTê°€ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ë©°, í•„ìš” ì‹œ ì§ˆë¬¸ì„ ë˜ì§€ê³  ê·¼ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
    
    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for msg in st.session_state.conversation:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    user_input = st.chat_input("ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
    if user_input:
        st.session_state.conversation.append({"role": "user", "content": user_input})
        system_prompt = {
            "role": "system",
            "content": "You are an assistant helping with document analysis. Answer the user's questions based on the provided document. Include direct quotes from the document as evidence when possible. If the user's question is unclear, ask a clarifying question."
        }
        messages = [system_prompt, {"role": "user", "content": f"Here is the document: {document_text}"}] + st.session_state.conversation
        response = ask_gpt(messages)
        st.session_state.conversation.append({"role": "assistant", "content": response})
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.conversation = []
        st.experimental_rerun()

###############################################################################
# ë©”ì¸ ì‹¤í–‰
###############################################################################
def main():
    st.title("ğŸ“š ThinHelper - ìƒê°ë„ìš°ë¯¸")

    st.markdown("""
    **ì´ ì•±ì€ íŒŒì¼ ì—…ë¡œë“œì™€ GPT ê¸°ë°˜ ë¬¸ì„œ ë¶„ì„ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.**
    
    - **íŒŒì¼ ì—…ë¡œë“œ:** PDF, PPTX, DOCX íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.
    - **ë¬¸ì„œ ë¶„ì„:** ë¬¸ì„œ ìš”ì•½, ì¤‘ìš” ë‹¨ì–´, ìˆ˜ì • ì œì•ˆ, ì§ˆë¬¸ì„ ì œê³µí•©ë‹ˆë‹¤.
    - **ëŒ€í™”í˜• ì±„íŒ…:** íŒŒì¼ ì—…ë¡œë“œ ì‹œ ìë™ìœ¼ë¡œ ì±„íŒ…ì°½ì´ ë‚˜íƒ€ë‚˜ë©°, ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  GPTì˜ ë‹µë³€ì„ ë°›ìŠµë‹ˆë‹¤.
    - **ì»¤ë®¤ë‹ˆí‹°:** ê²Œì‹œê¸€ ë“±ë¡, ê²€ìƒ‰, ëŒ“ê¸€ ê¸°ëŠ¥ì„ í†µí•´ ë¬¸ì„œë¥¼ ê³µìœ í•˜ê³  í† ë¡ í•©ë‹ˆë‹¤.
    """)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'uploaded_file' not in st.session_state:
        st.session_state.uploaded_file = None
    if 'document_text' not in st.session_state:
        st.session_state.document_text = None

    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("ğŸ“ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF/PPTX/DOCX ì§€ì›)", type=["pdf", "pptx", "docx"])

    if uploaded_file:
        if st.session_state.uploaded_file != uploaded_file:
            st.session_state.uploaded_file = uploaded_file
            file_bytes = uploaded_file.getvalue()
            fileinfo = {
                "name": uploaded_file.name,
                "ext": uploaded_file.name.split(".")[-1].lower(),
                "data": file_bytes
            }
            with st.spinner(f"ğŸ“– {fileinfo['name']} ë¶„ì„ ì¤‘..."):
                st.session_state.document_text = analyze_file(fileinfo)

    # íƒ­ ì„ íƒ
    tab = st.sidebar.radio("ğŸ” ë©”ë‰´ ì„ íƒ", ("GPT ë¬¸ì„œ ë¶„ì„", "ì»¤ë®¤ë‹ˆí‹°"))

    if tab == "GPT ë¬¸ì„œ ë¶„ì„":
        if st.session_state.document_text:
            # ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
            summary, questions, corrections = gpt_document_review(st.session_state.document_text)
            important_words = extract_important_words(st.session_state.document_text)
            st.subheader("ğŸ“Œ ì¤‘ìš” ë‹¨ì–´")
            st.write(", ".join(important_words))
            st.subheader("ğŸ“Œ ë¬¸ì„œ ìš”ì•½")
            st.write(summary)
            st.subheader("ğŸ’¡ ê³ ë ¤í•´ì•¼ í•  ì§ˆë¬¸")
            st.write(questions)
            st.subheader("âœï¸ ë§ì¶¤ë²• ë° ë¬¸ì¥ ìˆ˜ì •")
            st.write(corrections)
            
            # íŒŒì¼ ì—…ë¡œë“œ ì‹œ ìë™ìœ¼ë¡œ ëŒ€í™”í˜• ì±„íŒ…ì°½ í‘œì‹œ
            st.subheader("ğŸ’¬ ëŒ€í™”í˜• ì±„íŒ…")
            show_interactive_chat(st.session_state.document_text)
        else:
            st.info("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        community_tab()

if __name__ == "__main__":
    main()

###############################################################################
# ì»¤ë®¤ë‹ˆí‹° íƒ­ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
###############################################################################
def community_tab():
    st.header("ğŸŒ ì»¤ë®¤ë‹ˆí‹° (ë¬¸ì„œ ê³µìœ  ë° í† ë¡ )")
    st.info("""
    **[ì»¤ë®¤ë‹ˆí‹° ì‚¬ìš©ë²•]**
    1. ìƒë‹¨ì˜ ê²€ìƒ‰ì°½ì—ì„œ ì œëª© ë˜ëŠ” ë‚´ìš©ì„ ì…ë ¥í•˜ì—¬ ê¸°ì¡´ ê²Œì‹œê¸€ì„ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    2. 'ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì‘ì„±' ì˜ì—­ì—ì„œ ì œëª©, ë‚´ìš© ë° íŒŒì¼(PDF/PPTX/DOCX ì§€ì›)ì„ ì²¨ë¶€í•˜ì—¬ ê²Œì‹œê¸€ì„ ë“±ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    3. ê²Œì‹œê¸€ ìƒì„¸ë³´ê¸° ì˜ì—­ì—ì„œ ëŒ“ê¸€ì„ ì‘ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ëŒ“ê¸€ ì‘ì„± ì‹œ ì„ì˜ì˜ 'ìœ ì €_ìˆ«ì'ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.
    """)
    
    search_query = st.text_input("ğŸ” ê²€ìƒ‰ (ì œëª© ë˜ëŠ” ë‚´ìš© ì…ë ¥)")

    if "community_posts" not in st.session_state:
        st.session_state.community_posts = []

    st.subheader("ğŸ“¤ ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì‘ì„±")
    title = st.text_input("ì œëª©")
    content = st.text_area("ë‚´ìš©")
    uploaded_files = st.file_uploader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", type=["pdf", "pptx", "docx"], accept_multiple_files=True)

    if st.button("âœ… ê²Œì‹œê¸€ ë“±ë¡"):
        if title.strip() and content.strip():
            files_info = []
            if uploaded_files:
                for uf in uploaded_files:
                    file_bytes = uf.getvalue()
                    ext = uf.name.split(".")[-1].lower()
                    files_info.append({"name": uf.name, "ext": ext, "data": file_bytes})
            new_post = {"title": title, "content": content, "files": files_info, "comments": []}
            st.session_state.community_posts.append(new_post)
            st.success("âœ… ê²Œì‹œê¸€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")

    st.subheader("ğŸ“œ ê²Œì‹œê¸€ ëª©ë¡")
    for idx, post in enumerate(st.session_state.community_posts):
        if search_query.lower() in post["title"].lower() or search_query.lower() in post["content"].lower():
            with st.expander(f"{idx+1}. {post['title']}"):
                st.write(post["content"])
                comment = st.text_input(f"ğŸ’¬ ëŒ“ê¸€ ì‘ì„± (ì‘ì„±ì: ìœ ì €_{random.randint(100,999)})", key=f"comment_{idx}")
                if st.button("ëŒ“ê¸€ ë“±ë¡", key=f"comment_btn_{idx}"):
                    post["comments"].append(f"ğŸ“ ìœ ì €_{random.randint(100,999)}: {comment}")
                for c in post["comments"]:
                    st.write(c)

###############################################################################
# ì €ì‘ê¶Œ ì£¼ì˜ ë¬¸êµ¬ (Copyright Notice)
###############################################################################
"""
íŒŒì¼ ì—…ë¡œë“œ ì‹œ ì €ì‘ê¶Œì— ìœ ì˜í•´ì•¼ í•˜ë©°, ìš°ë¦¬ëŠ” ì´ ì½”ë“œì˜ ì‚¬ìš© ë˜ëŠ” ì—…ë¡œë“œëœ íŒŒì¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì–´ë– í•œ ì†í•´, ì˜¤ìš©, ì €ì‘ê¶Œ ì¹¨í•´ ë¬¸ì œì— ëŒ€í•´ ì±…ì„ì„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.

This source code is protected by copyright law. Unauthorized reproduction, distribution, modification, or commercial use is prohibited. 
It may only be used for personal, non-commercial purposes, and the source must be clearly credited upon use. 
Users must be mindful of copyright when uploading files, and we are not responsible for any damages, misuse, or copyright infringement issues arising from the use of this code or uploaded files.
"""

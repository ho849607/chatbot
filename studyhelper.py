import os
import streamlit as st
from io import BytesIO
from dotenv import load_dotenv

# OpenAI ëª¨ë“ˆì´ ì—†ì–´ë„ Gemini APIë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

from pathlib import Path
import docx2txt
import pdfplumber
from pptx import Presentation
import random
import subprocess
import nltk
from nltk.corpus import stopwords
import google.generativeai as genai
import PIL.Image
import requests
from concurrent.futures import ThreadPoolExecutor
import datetime
from PIL import Image
import io
from google.genai import types

###############################################################################
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ì„¤ì •
###############################################################################
dotenv_path = ".env"
load_dotenv(dotenv_path=dotenv_path)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Gemini API ì„¤ì •
genai.configure(api_key=GEMINI_API_KEY)

###############################################################################
# Streamlit ë° NLTK ì„¤ì •
###############################################################################
nltk_data_dir = "/tmp/nltk_data"
os.makedirs(nltk_data_dir, exist_ok=True)
nltk.data.path.append(nltk_data_dir)

@st.cache_data(show_spinner=False)
def get_stopwords():
    try:
        sw = stopwords.words("english")
    except LookupError:
        nltk.download("stopwords", download_dir=nltk_data_dir)
        sw = stopwords.words("english")
    return set(sw)

english_stopwords = get_stopwords()
korean_stopwords = {"ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë“¤", "ë°", "ë”"}
final_stopwords = english_stopwords.union(korean_stopwords)

###############################################################################
# ì˜¤ë²„í—¤ë“œ ê°ì†Œ: ìµœëŒ€í•œ ë¹ ë¥¸ ì²˜ë¦¬ ìœ„í•´ ë³‘ë ¬ ì œí•œ
###############################################################################
MAX_WORKERS = 2  # ë³‘ë ¬ ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ

###############################################################################
# OpenAIì™€ Gemini API í‚¤ ì„¤ì •
###############################################################################
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
USE_GEMINI_ALWAYS = os.getenv("USE_GEMINI_ALWAYS", "False").lower() == "true"

# OpenAI í´ë¼ì´ì–¸íŠ¸
if not OPENAI_API_KEY or OpenAI is None or USE_GEMINI_ALWAYS:
    # Gemini ê°•ì œ ì‚¬ìš©
    use_gemini_always = True
    client = None
else:
    use_gemini_always = False
    client = OpenAI(api_key=OPENAI_API_KEY, base_url="https://generativelanguage.googleapis.com/v1beta/openai/")

###############################################################################
# ìºì‹±ì„ í†µí•´ Gemini ì‘ë‹µ ì†ë„ ê°œì„ 
###############################################################################
@st.cache_data(show_spinner=False)
def ask_gemini_cached(prompt, temperature=0.2):
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(
            prompt=prompt,
            generation_config={"temperature": temperature}
        )
        return response.text.strip()
    except Exception as e:
        return f"Gemini API í˜¸ì¶œ ì˜¤ë¥˜: {e}"

###############################################################################
# OpenAI í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ Geminië¡œ fallback
###############################################################################
@st.cache_data(show_spinner=False)
def ask_gpt(messages, model_name="gpt-4", temperature=0.7):
    if use_gemini_always or client is None:
        # Gemini ì‚¬ìš© (fallback)
        return _ask_gemini(messages, temperature)
    else:
        # OpenAI ì‹œë„
        try:
            resp = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=temperature,
            )
            return resp.choices[0].message.content.strip()
        except Exception:
            return _ask_gemini(messages, temperature)

def _ask_gemini(messages, temperature=0.7):
    try:
        system_msg = next((m["content"] for m in messages if m["role"]=="system"), "")
        user_msg = next((m["content"] for m in messages if m["role"]=="user"), "")
        prompt = f"{system_msg}\n\nì‚¬ìš©ì ì§ˆë¬¸: {user_msg}"
        return ask_gemini_cached(prompt, temperature=temperature)
    except Exception as e:
        return f"Gemini API fallback ì˜¤ë¥˜: {e}"

###############################################################################
# ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ í›„ Geminiì— ì „ë‹¬ (ë¹ ë¥¸ ì†ë„)
###############################################################################
@st.cache_data(show_spinner=False)
def analyze_image_resized(file_bytes, max_size=(800, 800)):
    try:
        image = PIL.Image.open(io.BytesIO(file_bytes))
        image.thumbnail(max_size)
        buffer = io.BytesIO()
        image.save(buffer, format='JPEG')
        resized_image_bytes = buffer.getvalue()

        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(
            [types.Part.from_bytes(resized_image_bytes, mime_type='image/jpeg')],
            generation_config={"temperature": 0.2}
        )
        return response.text.strip()
    except Exception as e:
        return f"ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}"

###############################################################################
# ê¸°ë³¸ íŒŒì¼ íŒŒì‹± (ë¬¸ì„œ)
###############################################################################
@st.cache_data(show_spinner=False)
def parse_docx(file_bytes):
    try:
        return docx2txt.process(file_bytes)
    except Exception:
        return "ğŸ“„ DOCX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

@st.cache_data(show_spinner=False)
def parse_pdf(file_bytes):
    text_list = []
    try:
        with pdfplumber.open(file_bytes) as pdf:
            for page in pdf.pages:
                text = page.extract_text() or ""
                text_list.append(text)
        return "\n".join(text_list)
    except Exception:
        return "ğŸ“„ PDF íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

@st.cache_data(show_spinner=False)
def parse_ppt(file_bytes):
    text_list = []
    try:
        prs = Presentation(file_bytes)
        for slide in prs.slides:
            for shape in slide.shapes:
                if shape.has_text_frame:
                    text_list.append(shape.text)
        return "\n".join(text_list)
    except Exception:
        return "ğŸ“„ PPTX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

###############################################################################
def analyze_file(file_info):
    ext = file_info["ext"]
    data = file_info["data"]
    if ext == "docx":
        return parse_docx(BytesIO(data))
    elif ext == "pdf":
        return parse_pdf(BytesIO(data))
    elif ext == "pptx":
        return parse_ppt(BytesIO(data))
    elif ext in ["png", "jpg", "jpeg"]:
        # ë¦¬ì‚¬ì´ì¦ˆ í›„ ë¶„ì„
        return analyze_image_resized(data)
    else:
        return "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."

###############################################################################
# ì—¬ëŸ¬ íŒŒì¼ ë³‘í•© (ë³‘ë ¬ ì²˜ë¦¬)
###############################################################################
@st.cache_data(show_spinner=False)
def merge_documents(file_list):
    def process_file(f):
        bytes_data = f.getvalue()
        return analyze_file({
            "ext": f.name.split(".")[-1].lower(),
            "data": bytes_data
        })

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        results = list(executor.map(process_file, file_list))

    return "\n\n".join(results)

###############################################################################
# Gemini ìºì‹œ ê´€ë¦¬ í•¨ìˆ˜
###############################################################################
def manage_gemini_cache():
    from google import genai
    from google.genai import types

    try:
        cache_client = genai.Client(api_key=GEMINI_API_KEY)
        model_name = "models/gemini-1.5-flash-002"

        # ìºì‹œ ìƒì„±
        cache = cache_client.caches.create(
            model=model_name,
            config=types.CreateCachedContentConfig(contents=['hello'])
        )
        st.write("ìºì‹œ ìƒì„±ë¨:", cache)

        # ìºì‹œ TTL ì—…ë°ì´íŠ¸ (2ì‹œê°„)
        ttl_seconds = int(datetime.timedelta(hours=2).total_seconds())
        cache_client.caches.update(
            name=cache.name,
            config=types.UpdateCachedContentConfig(ttl=f"{ttl_seconds}s")
        )
        st.write("ìºì‹œ TTLì´ 2ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨.")

        # ìºì‹œ ì‚­ì œ
        cache_client.caches.delete(name=cache.name)
        st.write("ìºì‹œ ì‚­ì œë¨.")
    except Exception as e:
        st.error(f"Gemini ìºì‹œ ê´€ë¦¬ ì—ëŸ¬: {e}")

###############################################################################
# Streamlit UI
###############################################################################
def main():
    st.title("ğŸ“š Thinkhelper")
    st.markdown("""
**Thinkhelper**ëŠ” AI ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼(ë¬¸ì„œ/ì´ë¯¸ì§€)ì„ ë¹ ë¥´ê²Œ ë¶„ì„í•˜ê³ , ììœ ë¡­ê²Œ ëŒ€í™”í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

- íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ ê²°ê³¼(ìš”ì•½, ìˆ˜ì • ì œì•ˆ ë“±)ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- íŒŒì¼ ì—†ì´ë„ ììœ ë¡œìš´ AI ëŒ€í™” ê°€ëŠ¥í•©ë‹ˆë‹¤.
- ì»¤ë®¤ë‹ˆí‹° íƒ­ì—ì„œëŠ” ìµëª… ê²Œì‹œê¸€ ë° ëŒ“ê¸€ì„ í†µí•´ í˜‘ì—…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")

    # ì‚¬ì´ë“œë°”ì—ì„œ Gemini ìºì‹œ ê´€ë¦¬ ë²„íŠ¼
    if st.sidebar.button("Gemini Cache ê´€ë¦¬"):
        manage_gemini_cache()

    tab = st.sidebar.radio("ğŸ” ë©”ë‰´ ì„ íƒ", ("íŒŒì¼ ë¶„ì„ & GPT ì±„íŒ…", "ì»¤ë®¤ë‹ˆí‹°"))

    if tab == "íŒŒì¼ ë¶„ì„ & GPT ì±„íŒ…":
        st.info("íŒŒì¼(ë¬¸ì„œ, ì´ë¯¸ì§€)ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„í•˜ê³ , ì—…ë¡œë“œ ì—†ì´ë„ AIì™€ ëŒ€í™” ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        uploaded_files = st.file_uploader(
            "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ (PDF, PPTX, DOCX, ì´ë¯¸ì§€)",
            type=["pdf", "pptx", "docx", "png", "jpg", "jpeg"],
            accept_multiple_files=True
        )

        if "analysis_result" not in st.session_state:
            st.session_state.analysis_result = ""

        if uploaded_files:
            with st.spinner("íŒŒì¼ ë¶„ì„ ì¤‘..."):
                analysis = merge_documents(uploaded_files)
                st.session_state.analysis_result = analysis

            st.subheader("ğŸ“Œ ë¶„ì„ ê²°ê³¼")
            st.write(st.session_state.analysis_result)

        st.subheader("ğŸ’¬ GPTì™€ ëŒ€í™”í•˜ê¸°")
        user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
        if st.button("ì „ì†¡"):
            if user_input:
                prompt_context = st.session_state.analysis_result
                prompt = f"íŒŒì¼ ë‚´ìš©: {prompt_context}\nì§ˆë¬¸: {user_input}" if prompt_context else user_input

                # Gemini í˜¸ì¶œ (ìºì‹±)
                response = ask_gemini_cached(prompt, temperature=0.2)
                st.write("AI:", response)

    else:
        # ì»¤ë®¤ë‹ˆí‹° íƒ­
        st.info("ìµëª…ìœ¼ë¡œ ê²Œì‹œê¸€ ë° ëŒ“ê¸€ì„ ì‘ì„±í•˜ê³  í˜‘ì—…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        if "community_posts" not in st.session_state:
            st.session_state.community_posts = []

        search_query = st.text_input("ğŸ” ê²€ìƒ‰ (ì œëª© ë˜ëŠ” ë‚´ìš©)")
        st.subheader("ìƒˆ ê²Œì‹œê¸€ ì‘ì„±")
        title = st.text_input("ì œëª©")
        content = st.text_area("ë‚´ìš©")
        files_uploaded = st.file_uploader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ (ì„ íƒ)", type=["pdf", "pptx", "docx", "png", "jpg", "jpeg"], accept_multiple_files=True)

        if st.button("ê²Œì‹œê¸€ ë“±ë¡"):
            if title.strip() and content.strip():
                post_files = []
                if files_uploaded:
                    for uf in files_uploaded:
                        ext = uf.name.split(".")[-1].lower()
                        file_bytes = uf.getvalue()
                        post_files.append({"name": uf.name, "ext": ext, "data": file_bytes})
                new_post = {"title": title, "content": content, "files": post_files, "comments": []}
                st.session_state.community_posts.append(new_post)
                st.success("ê²Œì‹œê¸€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.error("ì œëª©ê³¼ ë‚´ìš©ì„ ëª¨ë‘ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")

        st.subheader("ğŸ“œ ê²Œì‹œê¸€ ëª©ë¡")
        for idx, post in enumerate(st.session_state.community_posts):
            if (not search_query) or (search_query.lower() in post["title"].lower() or search_query.lower() in post["content"].lower()):
                with st.expander(f"{idx+1}. {post['title']}"):
                    st.write(post["content"])
                    comment = st.text_input("ëŒ“ê¸€ ì‘ì„± (ìµëª…)", key=f"comment_{idx}")
                    if st.button("ëŒ“ê¸€ ë“±ë¡", key=f"comment_btn_{idx}"):
                        if comment.strip():
                            st.session_state.community_posts[idx]["comments"].append(f"ìµëª…_{random.randint(100,999)}: {comment}")
                        else:
                            st.error("ëŒ“ê¸€ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    for c in post["comments"]:
                        st.write(c)

if __name__ == "__main__":
    main()

st.markdown("""
---
**ì €ì‘ê¶Œ ì£¼ì˜ ë¬¸êµ¬**

- ë³¸ ì½”ë“œëŠ” ì €ì‘ê¶Œë²•ì— ì˜í•´ ë³´í˜¸ë©ë‹ˆë‹¤. ë¬´ë‹¨ ë³µì œ, ë°°í¬, ìˆ˜ì • ë˜ëŠ” ìƒì—…ì  ì‚¬ìš©ì€ ê¸ˆì§€ë©ë‹ˆë‹¤.
- ì‚¬ìš© ì‹œ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œê¸°í•´ì•¼ í•˜ë©°, ê°œì¸ì /ë¹„ìƒì—…ì  ìš©ë„ë¡œë§Œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
- íŒŒì¼ ì—…ë¡œë“œ ì‹œ ë°œìƒí•˜ëŠ” ì €ì‘ê¶Œ ì¹¨í•´ ë¬¸ì œì— ëŒ€í•´ì„œëŠ” ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
""")

import os
import streamlit as st
from io import BytesIO
from dotenv import load_dotenv

# OpenAI ëª¨ë“ˆì´ ì—†ì–´ë„ Gemini APIë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

from pathlib import Path
import docx2txt
import pdfplumber
from pptx import Presentation
import random
import subprocess
import nltk
from nltk.corpus import stopwords
import google.generativeai as genai
import PIL.Image
import requests
from concurrent.futures import ThreadPoolExecutor
import datetime
from PIL import Image
import io
from google.generativeai import types  # ì˜¬ë°”ë¥¸ ëª¨ë“ˆ ê²½ë¡œ ì‚¬ìš©
from docx import Document
import tempfile  # ì„ì‹œ íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ

###############################################################################
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ì„¤ì •
###############################################################################
dotenv_path = ".env"
load_dotenv(dotenv_path=dotenv_path)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
USE_GEMINI_ALWAYS = os.getenv("USE_GEMINI_ALWAYS", "False").lower() == "true"

# Gemini API ì„¤ì •
genai.configure(api_key=GEMINI_API_KEY)

###############################################################################
# Streamlit ë° NLTK ì„¤ì •
###############################################################################
nltk_data_dir = "/tmp/nltk_data"
os.makedirs(nltk_data_dir, exist_ok=True)
nltk.data.path.append(nltk_data_dir)

@st.cache_data(show_spinner=False)
def get_stopwords():
    try:
        sw = stopwords.words("english")
    except LookupError:
        nltk.download("stopwords", download_dir=nltk_data_dir)
        sw = stopwords.words("english")
    return set(sw)

english_stopwords = get_stopwords()
korean_stopwords = {"ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë“¤", "ë°", "ë”"}
final_stopwords = english_stopwords.union(korean_stopwords)

###############################################################################
# ë³‘ë ¬ ì œí•œ (ì„±ëŠ¥ ê°œì„ )
###############################################################################
MAX_WORKERS = 2

###############################################################################
# OpenAI & Gemini ì„¤ì •
###############################################################################
if not OPENAI_API_KEY or OpenAI is None or USE_GEMINI_ALWAYS:
    use_gemini_always = True
    openai_client = None
else:
    use_gemini_always = False
    # base_urlì€ ì˜ˆì‹œ. í•„ìš” ì‹œ "https://generativelanguage.googleapis.com/v1beta/openai/"
    openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url="https://api.openai.com/v1")

###############################################################################
# Gemini (ìºì‹± ì˜ˆì‹œ)
###############################################################################
@st.cache_data(show_spinner=False)
def ask_gemini_cached(prompt, temperature=0.2):
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        # promptë¥¼ í‚¤ì›Œë“œ ì¸ìê°€ ì•„ë‹ˆë¼ ìœ„ì¹˜ ì¸ìë¡œ ì „ë‹¬
        response = model.generate_content(
            prompt,
            generation_config={"temperature": temperature}
        )
        return response.text.strip()
    except Exception as e:
        return f"Gemini API í˜¸ì¶œ ì˜¤ë¥˜: {e}"

###############################################################################
# OpenAI í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ Geminië¡œ fallback
###############################################################################
@st.cache_data(show_spinner=False)
def ask_gpt(messages, model_name="gpt-4", temperature=0.2):
    """OpenAI GPT í˜¸ì¶œ. ì‹¤íŒ¨ ì‹œ Gemini APIë¡œ fallback."""
    if use_gemini_always or not openai_client:
        return _ask_gemini(messages, temperature=temperature)
    else:
        try:
            resp = openai_client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=temperature,
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            st.warning(f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}, Geminië¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            return _ask_gemini(messages, temperature=temperature)

def _ask_gemini(messages, temperature=0.2):
    try:
        system_msg = next((m["content"] for m in messages if m["role"]=="system"), "")
        user_msg = next((m["content"] for m in messages if m["role"]=="user"), "")
        prompt = f"{system_msg}\n\nì‚¬ìš©ì ì§ˆë¬¸: {user_msg}"
        return ask_gemini_cached(prompt, temperature=temperature)
    except Exception as e:
        st.error(f"Gemini API fallback ì˜¤ë¥˜: {e}")
        return "AI í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"

###############################################################################
# ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ í›„ ë¶„ì„ (ê°„ë‹¨ ì˜ˆì‹œ)
###############################################################################
@st.cache_data(show_spinner=False)
def analyze_image_resized(file_bytes, max_size=(800, 800)):
    try:
        image = PIL.Image.open(io.BytesIO(file_bytes))
        image.thumbnail(max_size)
        buffer = io.BytesIO()
        image.save(buffer, format='JPEG')
        resized_image_bytes = buffer.getvalue()

        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(
            [types.Part.from_bytes(resized_image_bytes, mime_type='image/jpeg')],
            generation_config={"temperature": 0.2}
        )
        return response.text.strip()
    except Exception as e:
        return f"ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}"

###############################################################################
# ë¬¸ì„œ íŒŒì‹± (PDF, DOCX, PPT)
###############################################################################
@st.cache_data(show_spinner=False)
def parse_docx(file_bytes):
    try:
        # BytesIO ê°ì²´ë¡œ ì „ë‹¬ëœ ë‚´ìš©ì„ ì„ì‹œ DOCX íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(suffix=".docx", delete=False) as tmp:
            content = file_bytes.getvalue() if hasattr(file_bytes, "getvalue") else file_bytes
            tmp.write(content)
            tmp_path = tmp.name
        text = docx2txt.process(tmp_path)
        os.remove(tmp_path)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        return text
    except Exception as e:
        return f"ğŸ“„ DOCX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜: {e}"

@st.cache_data(show_spinner=False)
def parse_pdf(file_bytes):
    text_list = []
    try:
        with pdfplumber.open(file_bytes) as pdf:
            for page in pdf.pages:
                text = page.extract_text() or ""
                text_list.append(text)
        return "\n".join(text_list)
    except Exception:
        return "ğŸ“„ PDF íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

@st.cache_data(show_spinner=False)
def parse_ppt(file_bytes):
    text_list = []
    try:
        prs = Presentation(file_bytes)
        for slide in prs.slides:
            for shape in slide.shapes:
                if shape.has_text_frame:
                    text_list.append(shape.text)
        return "\n".join(text_list)
    except Exception:
        return "ğŸ“„ PPTX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

def analyze_file(file_info):
    ext = file_info["ext"]
    data = file_info["data"]
    if ext == "docx":
        return parse_docx(BytesIO(data))
    elif ext == "pdf":
        return parse_pdf(BytesIO(data))
    elif ext == "pptx":
        return parse_ppt(BytesIO(data))
    elif ext in ["png", "jpg", "jpeg"]:
        return analyze_image_resized(data)
    else:
        return "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."

###############################################################################
# ì—¬ëŸ¬ íŒŒì¼ ë³‘í•© (ë³‘ë ¬ ì²˜ë¦¬)
###############################################################################
@st.cache_data(show_spinner=False)
def merge_documents(file_list):
    def process_file(f):
        bytes_data = f.getvalue()
        return analyze_file({
            "ext": f.name.split(".")[-1].lower(),
            "data": bytes_data
        })

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        results = list(executor.map(process_file, file_list))

    return "\n\n".join(results)

###############################################################################
# Gemini ìºì‹œ ê´€ë¦¬ í•¨ìˆ˜
###############################################################################
def manage_gemini_cache():
    from google.generativeai import types
    try:
        cache_client = genai.Client(api_key=GEMINI_API_KEY)
        model_name = "models/gemini-1.5-flash-002"

        # ìºì‹œ ìƒì„±
        cache = cache_client.caches.create(
            model=model_name,
            config=types.CreateCachedContentConfig(contents=['hello'])
        )
        st.write("ìºì‹œ ìƒì„±ë¨:", cache)

        # ìºì‹œ TTL ì—…ë°ì´íŠ¸ (2ì‹œê°„)
        ttl_seconds = int(datetime.timedelta(hours=2).total_seconds())
        cache_client.caches.update(
            name=cache.name,
            config=types.UpdateCachedContentConfig(ttl=f"{ttl_seconds}s")
        )
        st.write("ìºì‹œ TTLì´ 2ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨.")

        # ìºì‹œ ì‚­ì œ
        cache_client.caches.delete(name=cache.name)
        st.write("ìºì‹œ ì‚­ì œë¨.")
    except Exception as e:
        st.error(f"Gemini ìºì‹œ ê´€ë¦¬ ì—ëŸ¬: {e}")

###############################################################################
# ì»¤ë®¤ë‹ˆí‹° ë¬¸ì„œ êµ¬ì¡° ì˜ˆì‹œ (ë²„ì „ê´€ë¦¬)
# ì˜ˆì‹œ:
# st.session_state.community_posts = [
#     {
#       "id": 1,
#       "title": "ë²•ë¥ ë¬¸ì„œ ì´ˆì•ˆ",
#       "content": "ì—¬ê¸°ì— ë¬¸ì„œ ë³¸ë¬¸ì´ ë“¤ì–´ê°...",
#       "owner": "ìµëª…_101",
#       "history": [
#           {
#             "user": "ìµëª…_235",
#             "time": "2025-03-11 14:23",
#             "content": "ìˆ˜ì •ëœ ë¬¸ì„œ ë‚´ìš©..."
#           }
#       ]
#     },
#     ...
# ]
###############################################################################

def main():
    st.title("ğŸ“š Thinkhelper")
    st.markdown("""
**Thinkhelper**ëŠ” AI ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼(ë¬¸ì„œ/ì´ë¯¸ì§€)ì„ ë¹ ë¥´ê²Œ ë¶„ì„í•˜ê³ , ììœ ë¡­ê²Œ ëŒ€í™”í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

- íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ ê²°ê³¼(ìš”ì•½, ìˆ˜ì • ì œì•ˆ ë“±)ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- íŒŒì¼ ì—†ì´ë„ ììœ ë¡œìš´ AI ëŒ€í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- ì¶”ê°€ë¡œ, ìƒˆ ë¬¸ì„œë¥¼ AIë¡œ ìë™ ìƒì„±í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
- **ì»¤ë®¤ë‹ˆí‹° íƒ­ì—ì„œëŠ” ìµëª… ê²Œì‹œê¸€ì„ ì˜¬ë¦¬ê³ , ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ë¬¸ì„œë¥¼ ìˆ˜ì •**(ë²„ì „ê´€ë¦¬)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")

    if st.sidebar.button("Gemini Cache ê´€ë¦¬"):
        manage_gemini_cache()

    tab = st.sidebar.radio("ğŸ” ë©”ë‰´ ì„ íƒ", ("íŒŒì¼ ë¶„ì„ & GPT ì±„íŒ…", "AI ë¬¸ì„œ ìƒì„±", "ì»¤ë®¤ë‹ˆí‹°"))

    if "community_posts" not in st.session_state:
        # ì´ˆê¸° ì˜ˆì‹œ
        st.session_state.community_posts = []

    if tab == "íŒŒì¼ ë¶„ì„ & GPT ì±„íŒ…":
        st.info("íŒŒì¼(ë¬¸ì„œ, ì´ë¯¸ì§€)ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„. ì—…ë¡œë“œ ì—†ì´ë„ AIì™€ ëŒ€í™”.")
        uploaded_files = st.file_uploader(
            "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ (PDF, PPTX, DOCX, ì´ë¯¸ì§€)",
            type=["pdf", "pptx", "docx", "png", "jpg", "jpeg"],
            accept_multiple_files=True
        )

        if "analysis_result" not in st.session_state:
            st.session_state.analysis_result = ""

        if uploaded_files:
            with st.spinner("íŒŒì¼ ë¶„ì„ ì¤‘..."):
                analysis = merge_documents(uploaded_files)
                st.session_state.analysis_result = analysis

            st.subheader("ğŸ“Œ ë¶„ì„ ê²°ê³¼")
            st.write(st.session_state.analysis_result)

        st.subheader("ğŸ’¬ GPTì™€ ëŒ€í™”í•˜ê¸°")
        user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
        if st.button("ì „ì†¡"):
            if user_input:
                prompt_context = st.session_state.analysis_result
                messages = [
                    {"role": "system", "content": f"íŒŒì¼ ë‚´ìš©: {prompt_context}"},
                    {"role": "user", "content": user_input}
                ]
                response = ask_gpt(messages, model_name="gpt-4", temperature=0.2)
                st.write("AI:", response)

    elif tab == "AI ë¬¸ì„œ ìƒì„±":
        st.header("ğŸ“ ìƒˆ ë¬¸ì„œ ìƒì„± (ì‹¤ì‹œê°„ AI ì§€ì›)")
        st.markdown("ë¬¸ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë™ì‹œì— AIê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ë„ì›€ì„ ë“œë¦½ë‹ˆë‹¤.")
        # ë¬¸ì„œ ì‘ì„± ë° API ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ì´ˆê¸°ê°’ ì„¤ì •
        if "doc_text" not in st.session_state:
            st.session_state.doc_text = ""
        if "api_result" not in st.session_state:
            st.session_state.api_result = ""

        def update_document():
            user_input = st.session_state.doc_text
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": user_input}
            ]
            # ì‚¬ìš©ìê°€ ì‘ì„±í•œ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ API í˜¸ì¶œ
            result = ask_gpt(messages, model_name="gpt-4", temperature=0.2)
            st.session_state.api_result = result

        # on_change ì½œë°±ì„ í†µí•´ ì‚¬ìš©ìê°€ ë‚´ìš©ì„ ì‘ì„±í•  ë•Œë§ˆë‹¤ update_document í•¨ìˆ˜ê°€ í˜¸ì¶œë¨
        st.text_area("âœï¸ ë¬¸ì„œ ì‘ì„±", key="doc_text", height=400, on_change=update_document)
        st.subheader("AI ë„ì›€ ê²°ê³¼")
        st.write(st.session_state.get("api_result", ""))

        doc_type = st.selectbox("ë¬¸ì„œ í˜•ì‹ ì„ íƒ", ["DOCX (ì›Œë“œ ë¬¸ì„œ)", "í…ìŠ¤íŠ¸ íŒŒì¼ (.txt)"])
        if st.button("ğŸ“¥ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ"):
            if doc_type.startswith("DOCX"):
                doc = Document()
                doc.add_paragraph(st.session_state.doc_text)
                buffer = BytesIO()
                doc.save(buffer)
                buffer.seek(0)
                st.download_button(
                    label="DOCX ë‹¤ìš´ë¡œë“œ",
                    data=buffer,
                    file_name="generated_document.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                )
            else:
                st.download_button(
                    label="TXT ë‹¤ìš´ë¡œë“œ",
                    data=st.session_state.doc_text,
                    file_name="generated_document.txt",
                    mime="text/plain"
                )

    else:
        # ì»¤ë®¤ë‹ˆí‹° íƒ­ (ë²„ì „ ê´€ë¦¬: ëˆ„ê°€ ìˆ˜ì •í–ˆëŠ”ì§€ ê¸°ë¡)
        st.info("ìµëª…ìœ¼ë¡œ ê²Œì‹œê¸€(ë¬¸ì„œ) ë“±ë¡ í›„, ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ë¬¸ì„œë¥¼ ìˆ˜ì •í•˜ë©´ ë²„ì „ ê´€ë¦¬ ì´ë ¥ì„ ë‚¨ê¹ë‹ˆë‹¤.")

        search_query = st.text_input("ğŸ” ê²€ìƒ‰ (ì œëª© ë˜ëŠ” ë‚´ìš©)")
        st.subheader("ìƒˆ ê²Œì‹œê¸€(ë¬¸ì„œ) ì‘ì„±")
        title = st.text_input("ì œëª©")
        content = st.text_area("ë³¸ë¬¸")
        files_uploaded = st.file_uploader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ (ì„ íƒ)", type=["pdf", "pptx", "docx", "png", "jpg", "jpeg"], accept_multiple_files=True)

        if st.button("ê²Œì‹œê¸€ ë“±ë¡"):
            if title.strip() and content.strip():
                post_files = []
                if files_uploaded:
                    for uf in files_uploaded:
                        ext = uf.name.split(".")[-1].lower()
                        file_bytes = uf.getvalue()
                        post_files.append({"name": uf.name, "ext": ext, "data": file_bytes})
                new_id = len(st.session_state.community_posts) + 1
                new_post = {
                    "id": new_id,
                    "title": title,
                    "content": content,
                    "owner": f"ìµëª…_{random.randint(100,999)}",
                    "files": post_files,
                    "history": []  # ìˆ˜ì • ì´ë ¥
                }
                st.session_state.community_posts.append(new_post)
                st.success("ê²Œì‹œê¸€(ë¬¸ì„œ)ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.error("ì œëª©ê³¼ ë‚´ìš©ì„ ëª¨ë‘ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")

        st.subheader("ğŸ“œ ê²Œì‹œê¸€(ë¬¸ì„œ) ëª©ë¡")
        for idx, post in enumerate(st.session_state.community_posts):
            # ê²€ìƒ‰
            if (not search_query) or (search_query.lower() in post["title"].lower() or search_query.lower() in post["content"].lower()):
                with st.expander(f"{idx+1}. {post['title']}"):
                    st.write(post["content"])

                    # íŒŒì¼ ëª©ë¡ í‘œì‹œ
                    if post.get("files"):
                        st.markdown("**ì²¨ë¶€íŒŒì¼ ëª©ë¡**")
                        for f_idx, f_info in enumerate(post["files"]):
                            st.write(f"- {f_info['name']}")

                    # ìˆ˜ì •í•˜ê¸° ë²„íŠ¼
                    edit_key = f"edit_{post['id']}"
                    if st.button("ìˆ˜ì •í•˜ê¸°", key=f"edit_btn_{post['id']}"):
                        st.session_state[edit_key] = True

                    if edit_key in st.session_state and st.session_state[edit_key]:
                        st.mar

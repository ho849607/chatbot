import os
import streamlit as st
from io import BytesIO
from dotenv import load_dotenv
from pathlib import Path
import docx2txt
import pdfplumber
from pptx import Presentation
import nltk
from nltk.corpus import stopwords
import google.generativeai as genai
import PIL.Image
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
dotenv_path = ".env"
load_dotenv(dotenv_path)

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
genai.configure(api_key=GEMINI_API_KEY)

# ìºì‹±ì„ ì´ìš©í•œ ì„±ëŠ¥ ê°œì„ 
@st.cache_data(show_spinner=False)
def parse_docx(file_bytes):
    import docx2txt
    return docx2txt.process(file_bytes)

@st.cache_data(show_spinner=False)
def parse_pdf(file_bytes):
    import pdfplumber
    with pdfplumber.open(file_bytes) as pdf:
        return "\n".join(page.extract_text() for page in pdf.pages)

@st.cache_data(show_spinner=False)
def parse_ppt(file_bytes):
    from pptx import Presentation
    prs = Presentation(file_bytes)
    return "\n".join(shape.text for slide in prs.slides for shape in slide.shapes if shape.has_text_frame)

@st.cache_data(show_spinner=False)
def analyze_image(file_bytes):
    image = PIL.Image.open(file_bytes)
    image.thumbnail((512, 512))
    response = genai.GenerativeModel('gemini-1.5-flash').generate_content(
        ["Describe this image.", image],
        generation_config={"temperature": 0.2}
    )
    return response.text

def analyze_file(file):
    ext = file.name.split('.')[-1].lower()
    file_bytes = file.getvalue()
    if ext == "docx":
        return parse_docx(BytesIO(file_bytes))
    elif ext == "pdf":
        return parse_pdf(BytesIO(file_bytes))
    elif ext == "pptx":
        return parse_ppt(BytesIO(file_bytes))
    elif ext in ["png", "jpg", "jpeg"]:
        return analyze_image(BytesIO(file_bytes))
    return "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."

@st.cache_data(show_spinner=False)
def gemini_chat(prompt):
    response = genai.GenerativeModel('gemini-1.5-flash').generate_content(
        prompt,
        generation_config={"temperature": 0.2}
    )
    return response.text.strip()

# íŒŒì¼ ì²˜ë¦¬
@st.cache_data(show_spinner=False)
def merge_documents(file_list):
    def process_file(file):
        return analyze_file({
            "name": file.name,
            "ext": file.name.split(".")[-1].lower(),
            "data": file.getvalue()
        })

    with ThreadPoolExecutor(max_workers=4) as executor:
        results = executor.map(process_file, file_list)
    return "\n\n".join(results)

# Streamlit UI êµ¬ì„±
def main():
    st.title("ğŸ“š Thinkhelper")

    st.markdown("""
    **Thinkhelper**ëŠ” AI ê¸°ë°˜ íŒŒì¼(ë¬¸ì„œ ë° ì´ë¯¸ì§€) ë¶„ì„ê³¼ ììœ ë¡œìš´ ëŒ€í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

    **ì‚¬ìš©ë²•:**
    - íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¬¸ì„œ ë° ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - íŒŒì¼ ì—†ì´ë„ ììœ ë¡­ê²Œ ì§ˆë¬¸í•˜ê³  ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì»¤ë®¤ë‹ˆí‹° íƒ­ì—ì„œ ìµëª…ìœ¼ë¡œ ê²Œì‹œê¸€ ë° ëŒ“ê¸€ì„ í†µí•´ í˜‘ì—…ê³¼ í† ë¡ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """)

    tab = st.sidebar.radio("ğŸ” ë©”ë‰´ ì„ íƒ", ("íŒŒì¼ ë¶„ì„ & GPT ì±„íŒ…", "ì»¤ë®¤ë‹ˆí‹°"))

    if tab == "íŒŒì¼ ë¶„ì„ & GPT ì±„íŒ…":
        st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì§ì ‘ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì—¬ AIì™€ ëŒ€í™”í•˜ì„¸ìš”.")
        uploaded_files = st.file_uploader(
            "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ (PDF, PPTX, DOCX, ì´ë¯¸ì§€)",
            type=["pdf", "pptx", "docx", "png", "jpg", "jpeg"],
            accept_multiple_files=True
        )

        if uploaded_files:
            with st.spinner("íŒŒì¼ ë¶„ì„ ì¤‘..."):
                analysis_result = merge_documents(uploaded_files)
                st.session_state.analysis_result = analysis_result

            st.subheader("ğŸ“Œ ë¶„ì„ ê²°ê³¼")
            st.write(st.session_state.analysis_result)

        st.subheader("ğŸ’¬ GPTì™€ ëŒ€í™”í•˜ê¸°")
        user_input = st.text_input("ì§ˆë¬¸ ì…ë ¥")

        if st.button("ì „ì†¡"):
            if user_input:
                prompt_context = st.session_state.get('analysis_result', '')
                prompt = f"íŒŒì¼ ë‚´ìš©: {prompt_context}\nì‚¬ìš©ì ì§ˆë¬¸: {user_input}"
                response = gemini_chat(prompt)
                st.write(f"AI: {response}")

    elif tab == "ì»¤ë®¤ë‹ˆí‹°":
        st.info("ì»¤ë®¤ë‹ˆí‹° ê¸°ëŠ¥ì€ ìµëª…ìœ¼ë¡œ ê²Œì‹œê¸€ê³¼ ëŒ“ê¸€ì„ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # ì—¬ê¸°ì— ì»¤ë®¤ë‹ˆí‹° ê¸°ëŠ¥ ì¶”ê°€ êµ¬í˜„ ê°€ëŠ¥

if __name__ == "__main__":
    main()

st.markdown("""
---
**ì €ì‘ê¶Œ ì£¼ì˜ ë¬¸êµ¬**
- ë³¸ ì½”ë“œì™€ ì„œë¹„ìŠ¤ ì‚¬ìš© ì‹œ ë°œìƒí•˜ëŠ” ì €ì‘ê¶Œ ë¬¸ì œì— ëŒ€í•œ ì±…ì„ì€ ì‚¬ìš©ìì—ê²Œ ìˆìŠµë‹ˆë‹¤.
- ê°œì¸ì , ë¹„ìƒì—…ì  ìš©ë„ë¡œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")

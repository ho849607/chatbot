import os
import streamlit as st
from io import BytesIO
from dotenv import load_dotenv
from openai import OpenAI
from pathlib import Path
import hashlib
import base64
import random
import subprocess

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import docx2txt
import pdfplumber
from pptx import Presentation

###############################################################################
# NLTK ì„¤ì • (í•„ìš” ì‹œ)
###############################################################################
nltk_data_dir = "/tmp/nltk_data"
os.makedirs(nltk_data_dir, exist_ok=True)
os.environ["NLTK_DATA"] = nltk_data_dir
nltk.data.path.append(nltk_data_dir)

try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt", download_dir=nltk_data_dir)
try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords", download_dir=nltk_data_dir)

korean_stopwords = ["ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë“¤", "ë°", "ë”"]
english_stopwords = set(stopwords.words("english"))
final_stopwords = english_stopwords.union(set(korean_stopwords))

###############################################################################
# í™˜ê²½ ë³€ìˆ˜ & OpenAI API
###############################################################################
dotenv_path = Path(".env")
load_dotenv(dotenv_path=dotenv_path)

# ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ì§ì ‘ í‚¤ í•˜ë“œì½”ë”©í•´ë„ ë˜ì§€ë§Œ, ë³´ì•ˆìƒ .env íŒŒì¼ ì‚¬ìš© ê¶Œì¥
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("ì„œë²„ì— OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=OPENAI_API_KEY)

###############################################################################
# OpenAI API ë§ˆì´ê·¸ë ˆì´ì…˜ (ì˜ˆì „ ë²„ì „ í˜¸í™˜ - í•„ìš” ì‹œ)
###############################################################################
def migrate_openai_api():
    try:
        subprocess.run(["openai", "migrate"], capture_output=True, text=True, check=True)
        st.info("OpenAI API ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ. ì•± ì¬ì‹œì‘ í›„ ì‚¬ìš©í•˜ì„¸ìš”.")
        st.stop()
    except Exception as e:
        st.error(f"API ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
        st.stop()

###############################################################################
# GPT í•¨ìˆ˜
###############################################################################
def ask_gpt(messages, model_name="gpt-4", temperature=0.7):
    try:
        resp = client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=temperature,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        error_message = str(e)
        if "no longer supported" in error_message:
            migrate_openai_api()
        st.error(f"OpenAI API í˜¸ì¶œ ì—ëŸ¬: {e}")
        return ""

###############################################################################
# íŒŒì¼ ë¶„ì„ ë¡œì§
###############################################################################
def parse_docx(file_bytes):
    try:
        return docx2txt.process(BytesIO(file_bytes))
    except Exception:
        return "DOCX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

def parse_pdf(file_bytes):
    text_list = []
    try:
        with pdfplumber.open(BytesIO(file_bytes)) as pdf:
            for page in pdf.pages:
                text_list.append(page.extract_text() or "")
        return "\n".join(text_list)
    except Exception:
        return "PDF íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

def parse_ppt(file_bytes):
    text_list = []
    try:
        prs = Presentation(BytesIO(file_bytes))
        for slide in prs.slides:
            for shape in slide.shapes:
                if shape.has_text_frame:
                    text_list.append(shape.text)
        return "\n".join(text_list)
    except Exception:
        return "PPTX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

def parse_image(file_bytes):
    """OCR ë¯¸êµ¬í˜„ - ë‹¨ìˆœ ì•ˆë‚´ë¬¸ ë°˜í™˜"""
    return "[ì´ë¯¸ì§€ íŒŒì¼] OCR ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€ ê°€ëŠ¥"

def analyze_file(fileinfo):
    """íŒŒì¼ í™•ì¥ìì— ë§ì¶° íŒŒì‹± í•¨ìˆ˜ í˜¸ì¶œ"""
    ext = fileinfo["ext"]
    data = fileinfo["data"]
    if ext == "docx":
        return parse_docx(data)
    elif ext == "pdf":
        return parse_pdf(data)
    elif ext == "pptx":
        return parse_ppt(data)
    elif ext in ["jpg", "jpeg", "png"]:
        return parse_image(data)
    else:
        return "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."

###############################################################################
# GPT ì±„íŒ… íƒ­
###############################################################################
def gpt_chat_tab():
    st.header("ğŸ“Œ GPT ì±„íŒ…")
    st.info("""
    **[GPT ì±„íŒ… ì‚¬ìš©ë²•]**
    1. ì•„ë˜ì˜ íŒŒì¼ ì—…ë¡œë“œ ì˜ì—­ì—ì„œ PDF/PPTX/DOCX/ì´ë¯¸ì§€(JPG/PNG) íŒŒì¼ì„ ì„ íƒí•˜ë©´ ìë™ìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤.
    2. ë¶„ì„ ê²°ê³¼ëŠ” ì±„íŒ… í˜•ì‹ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.
    3. ë©”ì‹œì§€ ì…ë ¥ë€ì— ì§ˆë¬¸ì„ ì‘ì„±í•˜ë©´ GPTê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    """)

    # ê¸°ì¡´ ì±„íŒ… ê¸°ë¡
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []

    for msg in st.session_state.chat_messages:
        role, content = msg["role"], msg["content"]
        with st.chat_message(role):
            st.write(content)

    # ------------------------- 1) ì¼ë°˜ íŒŒì¼ ì—…ë¡œë“œ -----------------------------
    uploaded_files = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ (PDF/PPTX/DOCX/JPG/PNG)",
        type=["pdf", "pptx", "docx", "jpg", "png"],
        accept_multiple_files=True
    )
    if uploaded_files:
        for uf in uploaded_files:
            file_bytes = uf.getvalue()
            fileinfo = {
                "name": uf.name,
                "ext": uf.name.split(".")[-1].lower(),
                "data": file_bytes
            }
            with st.spinner(f"ğŸ“– {fileinfo['name']} ë¶„ì„ ì¤‘..."):
                analysis_result = analyze_file(fileinfo)
            st.session_state.chat_messages.append({"role": "system", "content": f"ğŸ“„ {fileinfo['name']} ë¶„ì„ ì™„ë£Œ."})
            st.session_state.chat_messages.append({"role": "assistant", "content": analysis_result})

    # ------------------------- 2) ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ------------------------------
    user_msg = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    if user_msg:
        st.session_state.chat_messages.append({"role": "user", "content": user_msg})
        with st.chat_message("user"):
            st.write(user_msg)

        with st.spinner("GPT ì‘ë‹µ ì¤‘..."):
            gpt_response = ask_gpt(st.session_state.chat_messages)
        st.session_state.chat_messages.append({"role": "assistant", "content": gpt_response})
        with st.chat_message("assistant"):
            st.write(gpt_response)

###############################################################################
# ì»¤ë®¤ë‹ˆí‹° íƒ­
###############################################################################
def community_tab():
    st.header("ğŸŒ ì»¤ë®¤ë‹ˆí‹° (ë¬¸ì„œ ê³µìœ  ë° í† ë¡ )")
    st.info("""
    **[ì»¤ë®¤ë‹ˆí‹° ì‚¬ìš©ë²•]**
    1. ìƒë‹¨ì˜ ê²€ìƒ‰ì°½ì—ì„œ ì œëª© ë˜ëŠ” ë‚´ìš©ì„ ì…ë ¥í•˜ì—¬ ê¸°ì¡´ ê²Œì‹œê¸€ì„ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    2. 'ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì‘ì„±' ì˜ì—­ì—ì„œ ì œëª©, ë‚´ìš© ë° íŒŒì¼(PDF/PPTX/DOCX/JPG/PNG)ì„ ì²¨ë¶€í•˜ì—¬ ê²Œì‹œê¸€ì„ ë“±ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    3. ê²Œì‹œê¸€ ìƒì„¸ë³´ê¸° ì˜ì—­ì—ì„œ ëŒ“ê¸€ì„ ì‘ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ëŒ“ê¸€ ì‘ì„± ì‹œ ì„ì˜ì˜ 'ìœ ì €_ìˆ«ì'ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.
    """)
    
    search_query = st.text_input("ğŸ” ê²€ìƒ‰ (ì œëª© ë˜ëŠ” ë‚´ìš© ì…ë ¥)")

    if "community_posts" not in st.session_state:
        st.session_state.community_posts = []

    st.subheader("ğŸ“¤ ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì‘ì„±")
    title = st.text_input("ì œëª©")
    content = st.text_area("ë‚´ìš©")
    uploaded_files = st.file_uploader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", type=["pdf","pptx","docx","jpg","png"], accept_multiple_files=True)

    if st.button("ê²Œì‹œê¸€ ë“±ë¡"):
        if title.strip() and content.strip():
            files_info = []
            if uploaded_files:
                for uf in uploaded_files:
                    file_bytes = uf.getvalue()
                    ext = uf.name.split(".")[-1].lower()
                    files_info.append({"name":uf.name, "ext":ext, "data":file_bytes})
            new_post = {"title": title, "content": content, "files": files_info, "comments": []}
            st.session_state.community_posts.append(new_post)
            st.success("âœ… ê²Œì‹œê¸€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")

    st.subheader("ğŸ“œ ê²Œì‹œê¸€ ëª©ë¡")
    for idx, post in enumerate(st.session_state.community_posts):
        # ê²€ìƒ‰
        if search_query.lower() in post["title"].lower() or search_query.lower() in post["content"].lower():
            with st.expander(f"{idx+1}. {post['title']}"):
                st.write(post["content"])
                comment = st.text_input(f"ğŸ’¬ ëŒ“ê¸€ ì‘ì„± (ì‘ì„±ì: ìœ ì €_{random.randint(100,999)})", key=f"comment_{idx}")
                if st.button("ëŒ“ê¸€ ë“±ë¡", key=f"comment_btn_{idx}"):
                    post["comments"].append(f"ğŸ“ ìœ ì €_{random.randint(100,999)}: {comment}")
                for c in post["comments"]:
                    st.write(c)

###############################################################################
# ë©”ì¸ ì‹¤í–‰
###############################################################################
def main():
    st.title("ğŸ“š StudyHelper")

    st.markdown("""
    **ì´ ì•±ì€ íŒŒì¼ ì—…ë¡œë“œì™€ GPT ì±„íŒ… ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.**
    
    - **GPT ì±„íŒ… íƒ­**ì—ì„œ:
      1. íŒŒì¼ ì—…ë¡œë“œ (PDF/PPTX/DOCX/JPG/PNG)
      2. ë¶„ì„ ê²°ê³¼ë¥¼ ì±„íŒ… í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
      3. ë©”ì‹œì§€ ì…ë ¥(ì±„íŒ…)ì„ í†µí•´ GPT ëŒ€í™”
    - **ì»¤ë®¤ë‹ˆí‹° íƒ­**: ê¸°ì¡´ ê²Œì‹œê¸€ ë“±ë¡/ê²€ìƒ‰/ëŒ“ê¸€ ê¸°ëŠ¥
    """)
    
    tab = st.sidebar.radio("ğŸ” ë©”ë‰´ ì„ íƒ", ("GPT ì±„íŒ…", "ì»¤ë®¤ë‹ˆí‹°"))
    if tab == "GPT ì±„íŒ…":
        gpt_chat_tab()
    else:
        community_tab()

if __name__ == "__main__":
    main()

###############################################################################
# ì €ì‘ê¶Œ ì£¼ì˜ ë¬¸êµ¬ (Copyright Notice)
###############################################################################
"""
[í•œêµ­ì–´]
ì´ ì†ŒìŠ¤ ì½”ë“œëŠ” ì €ì‘ê¶Œë²•ì— ì˜í•´ ë³´í˜¸ë©ë‹ˆë‹¤. ë¬´ë‹¨ ë³µì œ, ë°°í¬, ìˆ˜ì • ë˜ëŠ” ìƒì—…ì  ì‚¬ìš©ì€ ê¸ˆì§€ë©ë‹ˆë‹¤. 
ê°œì¸ì ì´ê³  ë¹„ìƒì—…ì ì¸ ìš©ë„ë¡œë§Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë©°, ì‚¬ìš© ì‹œ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œê¸°í•´ì•¼ í•©ë‹ˆë‹¤. 
ë¬¸ì˜: (ì—°ë½ì²˜ë¥¼ ì—¬ê¸°ì— ì¶”ê°€í•˜ì„¸ìš”).

[English]
This source code is protected by copyright law. Unauthorized reproduction, distribution, modification, or commercial use is prohibited. 
It may only be used for personal, non-commercial purposes, and the source must be clearly credited upon use. 
For inquiries: (Add your contact information here).
"""

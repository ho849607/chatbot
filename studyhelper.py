import os
import streamlit as st
from io import BytesIO
from dotenv import load_dotenv
import openai
from pathlib import Path
import hashlib
import base64
import random
import subprocess

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import docx2txt
import pdfplumber
from pptx import Presentation

###############################################################################
# NLTK ì„¤ì • (í•„ìš” ì‹œ)
###############################################################################
nltk_data_dir = "/tmp/nltk_data"
os.makedirs(nltk_data_dir, exist_ok=True)
os.environ["NLTK_DATA"] = nltk_data_dir
nltk.data.path.append(nltk_data_dir)

try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt", download_dir=nltk_data_dir)

try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords", download_dir=nltk_data_dir)

korean_stopwords = ["ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë“¤", "ë°", "ë”"]
english_stopwords = set(stopwords.words("english"))
final_stopwords = english_stopwords.union(set(korean_stopwords))

###############################################################################
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ & OpenAI API í‚¤ ì„¤ì •
###############################################################################
dotenv_path = Path(".env")
load_dotenv(dotenv_path=dotenv_path)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # .env íŒŒì¼ì—ì„œ KEYë¥¼ ê°€ì ¸ì˜´
if not OPENAI_API_KEY:
    st.error("ì„œë²„ì— OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

# ìµœì‹  OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°©ì‹ - ì „ì—­ìœ¼ë¡œ API í‚¤ ì„¤ì •
openai.api_key = OPENAI_API_KEY

###############################################################################
# (ì„ íƒ) OpenAI êµ¬ë²„ì „ ë§ˆì´ê·¸ë ˆì´ì…˜ í•¨ìˆ˜
###############################################################################
def migrate_openai_api():
    """
    'no longer supported' ì˜¤ë¥˜ ë°œìƒ ì‹œ 'openai migrate' ëª…ë ¹ì„ ì‹œë„í•´ ë³´ëŠ” í•¨ìˆ˜.
    ë³´í†µ openai>=1.0.0 í™˜ê²½ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨.
    """
    try:
        subprocess.run(["openai", "migrate"], capture_output=True, text=True, check=True)
        st.info("OpenAI API ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ. ì•±ì„ ì¬ì‹œì‘í•´ ì£¼ì„¸ìš”.")
        st.stop()
    except Exception as e:
        st.error(f"API ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}\n'openai migrate' ëª…ë ¹ì„ í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ ë³´ì„¸ìš”.")
        st.stop()

###############################################################################
# GPT í•¨ìˆ˜ (ChatCompletion)
###############################################################################
def ask_gpt(messages, model_name="gpt-4", temperature=0.7):
    """
    OpenAI ChatCompletion APIë¥¼ í†µí•´ GPT ëª¨ë¸ì—ê²Œ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ê³  ì‘ë‹µì„ ë°›ëŠ” í•¨ìˆ˜.
    """
    try:
        resp = openai.ChatCompletion.create(
            model=model_name,
            messages=messages,
            temperature=temperature,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        error_message = str(e)
        if "no longer supported" in error_message:
            migrate_openai_api()
        st.error(f"OpenAI API í˜¸ì¶œ ì—ëŸ¬: {e}")
        return ""

###############################################################################
# íŒŒì¼ ë¶„ì„ ë¡œì§ (PDF, PPTX, DOCX, ì´ë¯¸ì§€)
###############################################################################
def parse_docx(file_bytes):
    try:
        return docx2txt.process(BytesIO(file_bytes))
    except Exception:
        return "DOCX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

def parse_pdf(file_bytes):
    text_list = []
    try:
        with pdfplumber.open(BytesIO(file_bytes)) as pdf:
            for page in pdf.pages:
                text = page.extract_text() or ""
                text_list.append(text)
        return "\n".join(text_list)
    except Exception:
        return "PDF íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

def parse_ppt(file_bytes):
    text_list = []
    try:
        prs = Presentation(BytesIO(file_bytes))
        for slide in prs.slides:
            for shape in slide.shapes:
                if shape.has_text_frame:
                    text_list.append(shape.text)
        return "\n".join(text_list)
    except Exception:
        return "PPTX íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜"

def parse_image(file_bytes):
    """ì´ë¯¸ì§€ íŒŒì¼ì— ëŒ€í•œ OCR ë¶„ì„ ë¡œì§ (í˜„ì¬ëŠ” ë‹¨ìˆœ ì•ˆë‚´ë§Œ ë°˜í™˜)"""
    return "[ì´ë¯¸ì§€ íŒŒì¼] OCR ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€ ê°€ëŠ¥"

def analyze_file(fileinfo):
    """
    ì—…ë¡œë“œëœ íŒŒì¼ì˜ í™•ì¥ìë¥¼ í™•ì¸í•œ ë’¤,
    í•´ë‹¹ íŒŒì¼ì— ë§ëŠ” íŒŒì‹± í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ.
    """
    ext = fileinfo["ext"]
    data = fileinfo["data"]

    if ext == "docx":
        return parse_docx(data)
    elif ext == "pdf":
        return parse_pdf(data)
    elif ext == "pptx":
        return parse_ppt(data)
    elif ext in ["jpg", "jpeg", "png"]:
        return parse_image(data)
    else:
        return "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."

###############################################################################
# GPT ì±„íŒ… íƒ­
###############################################################################
def gpt_chat_tab():
    st.header("ğŸ“Œ GPT ì±„íŒ…")
    st.info("""
    **[GPT ì±„íŒ… ì‚¬ìš©ë²•]**
    1. ì•„ë˜ì˜ íŒŒì¼ ì—…ë¡œë“œ ì˜ì—­ì—ì„œ PDF/PPTX/DOCX/ì´ë¯¸ì§€(JPG/PNG) íŒŒì¼ì„ ì„ íƒí•˜ë©´ ìë™ìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤.
    2. ë¶„ì„ ê²°ê³¼ëŠ” ì±„íŒ… í˜•ì‹ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.
    3. ë©”ì‹œì§€ ì…ë ¥ë€ì— ì§ˆë¬¸ì„ ì‘ì„±í•˜ë©´ GPTê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    """)

    # ì±„íŒ… ë©”ì‹œì§€ ì„¸ì…˜ ì´ˆê¸°í™”
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []

    # ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
    for msg in st.session_state.chat_messages:
        role, content = msg["role"], msg["content"]
        with st.chat_message(role):
            st.write(content)

    # íŒŒì¼ ì—…ë¡œë“œ í›„ ë¶„ì„
    uploaded_files = st.file_uploader(
        "íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF/PPTX/DOCX/JPG/PNG)",
        type=["pdf", "pptx", "docx", "jpg", "png"],
        accept_multiple_files=True
    )
    if uploaded_files:
        for uf in uploaded_files:
            file_bytes = uf.getvalue()
            fileinfo = {
                "name": uf.name,
                "ext": uf.name.split(".")[-1].lower(),
                "data": file_bytes
            }
            with st.spinner(f"ğŸ“– {fileinfo['name']} ë¶„ì„ ì¤‘..."):
                analysis_result = analyze_file(fileinfo)

            # ë¶„ì„ ê²°ê³¼ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
            st.session_state.chat_messages.append({"role": "system", "content": f"ğŸ“„ {fileinfo['name']} ë¶„ì„ ì™„ë£Œ."})
            st.session_state.chat_messages.append({"role": "assistant", "content": analysis_result})

    # ì‚¬ìš©ì ì…ë ¥
    user_msg = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    if user_msg:
        st.session_state.chat_messages.append({"role": "user", "content": user_msg})
        with st.chat_message("user"):
            st.write(user_msg)

        # GPT ì‘ë‹µ í˜¸ì¶œ
        with st.spinner("GPT ì‘ë‹µ ì¤‘..."):
            gpt_response = ask_gpt(st.session_state.chat_messages)

        st.session_state.chat_messages.append({"role": "assistant", "content": gpt_response})
        with st.chat_message("assistant"):
            st.write(gpt_response)

###############################################################################
# ì»¤ë®¤ë‹ˆí‹° íƒ­
###############################################################################
def community_tab():
    st.header("ğŸŒ ì»¤ë®¤ë‹ˆí‹° (ë¬¸ì„œ ê³µìœ  ë° í† ë¡ )")
    st.info("""
    **[ì»¤ë®¤ë‹ˆí‹° ì‚¬ìš©ë²•]**
    1. ìƒë‹¨ì˜ ê²€ìƒ‰ì°½ì—ì„œ ì œëª© ë˜ëŠ” ë‚´ìš©ì„ ì…ë ¥í•˜ì—¬ ê¸°ì¡´ ê²Œì‹œê¸€ì„ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    2. 'ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì‘ì„±' ì˜ì—­ì—ì„œ ì œëª©, ë‚´ìš© ë° íŒŒì¼(PDF/PPTX/DOCX/JPG/PNG)ì„ ì²¨ë¶€í•˜ì—¬ ê²Œì‹œê¸€ì„ ë“±ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    3. ê²Œì‹œê¸€ ìƒì„¸ë³´ê¸° ì˜ì—­ì—ì„œ ëŒ“ê¸€ì„ ì‘ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ëŒ“ê¸€ ì‘ì„± ì‹œ ì„ì˜ì˜ 'ìœ ì €_ìˆ«ì'ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.
    """)

    # ê²€ìƒ‰ ê¸°ëŠ¥
    search_query = st.text_input("ğŸ” ê²€ìƒ‰ (ì œëª© ë˜ëŠ” ë‚´ìš© ì…ë ¥)")

    # ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€ ì´ˆê¸°í™”
    if "community_posts" not in st.session_state:
        st.session_state.community_posts = []

    st.subheader("ğŸ“¤ ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì‘ì„±")
    title = st.text_input("ì œëª©")
    content = st.text_area("ë‚´ìš©")
    uploaded_files = st.file_uploader(
        "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ (PDF/PPTX/DOCX/JPG/PNG)",
        type=["pdf", "pptx", "docx", "jpg", "png"],
        accept_multiple_files=True
    )

    # ê²Œì‹œê¸€ ë“±ë¡
    if st.button("ê²Œì‹œê¸€ ë“±ë¡"):
        if title.strip() and content.strip():
            files_info = [
                {
                    "name": uf.name,
                    "ext": uf.name.split(".")[-1].lower(),
                    "data": uf.getvalue()
                }
                for uf in uploaded_files
            ] if uploaded_files else []
            new_post = {
                "title": title,
                "content": content,
                "files": files_info,
                "comments": []
            }
            st.session_state.community_posts.append(new_post)
            st.success("âœ… ê²Œì‹œê¸€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")

    st.subheader("ğŸ“œ ê²Œì‹œê¸€ ëª©ë¡")
    for idx, post in enumerate(st.session_state.community_posts):
        # ê²€ìƒ‰ ì¡°ê±´
        if search_query.lower() in post["title"].lower() or search_query.lower() in post["content"].lower():
            with st.expander(f"{idx+1}. {post['title']}"):
                st.write(post["content"])

                # ëŒ“ê¸€ ì‘ì„±
                comment_key = f"comment_box_{idx}"
                comment_btn_key = f"comment_btn_{idx}"
                comment = st.text_input(
                    f"ğŸ’¬ ëŒ“ê¸€ ì‘ì„± (ì‘ì„±ì: ìœ ì €_{random.randint(100,999)})",
                    key=comment_key
                )
                if st.button("ëŒ“ê¸€ ë“±ë¡", key=comment_btn_key):
                    post["comments"].append(
                        f"ğŸ“ ìœ ì €_{random.randint(100,999)}: {comment}"
                    )

                # ëŒ“ê¸€ ëª©ë¡
                for c in post["comments"]:
                    st.write(c)

###############################################################################
# ë©”ì¸ ì‹¤í–‰
###############################################################################
def main():
    st.title("ğŸ“š StudyHelper")

    st.markdown("""
    ## StudyHelper ì‚¬ìš©ë²• ì•ˆë‚´
    - **GPT ì±„íŒ…:** íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ AIê°€ ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ë©°, ë°”ë¡œ AIì™€ ëŒ€í™”ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - **ì»¤ë®¤ë‹ˆí‹°:** ììœ ë¡­ê²Œ ê²Œì‹œê¸€ì„ ì‘ì„±í•˜ê³ , ì„œë¡œ ì˜ê²¬ì„ ì£¼ê³ ë°›ìœ¼ë©° í† ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    **ì£¼ì˜ì‚¬í•­**
    - **ì €ì‘ê¶Œ ì•ˆë‚´:** ì—…ë¡œë“œí•˜ì‹  íŒŒì¼ ë° ì½˜í…ì¸ ëŠ” ì €ì‘ê¶Œ ë³´í˜¸ ëŒ€ìƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      ë³¸ í”Œë«í¼ì€ ìë£Œì— ëŒ€í•œ ì €ì‘ê¶Œ ì±…ì„ì„ ì§€ì§€ ì•Šìœ¼ë¯€ë¡œ, ì—…ë¡œë“œ ì „ ê´€ë ¨ ë²•ê·œë¥¼ ì¤€ìˆ˜í•´ ì£¼ì„¸ìš”.
    - **ì¤‘ìš” ì •ë³´ í™•ì¸:** ChatGPTì˜ ë‹µë³€ì€ ì°¸ê³ ìš©ì´ë©°, ì˜¤ë¥˜ë‚˜ ë¶€ì •í™•í•œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      ì¤‘ìš”í•œ ê²°ì •ì„ ë‚´ë¦´ ë•ŒëŠ” ë°˜ë“œì‹œ ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.
    """)

    # ì‚¬ì´ë“œë°” ë©”ë‰´
    tab = st.sidebar.radio("ğŸ” ë©”ë‰´ ì„ íƒ", ("GPT ì±„íŒ…", "ì»¤ë®¤ë‹ˆí‹°"))
    if tab == "GPT ì±„íŒ…":
        gpt_chat_tab()
    else:
        community_tab()

if __name__ == "__main__":
    main()
